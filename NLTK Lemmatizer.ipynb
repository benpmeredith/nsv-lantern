{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\n",
    "file_directory = 'C:/Users/danil/Documents/GitHub/Lantern_v2-master/Lantern_v2-master/Data'\n",
    "file_folders = ['/bully_data','/drug_data','/fight_assault_data','/SH_data','/vandalism_data']\n",
    "# file = '/a_teacher_focused_approach.txt'\n",
    "\n",
    "# file_1 = open(file_directory+file_folders[0]+file,'r',encoding = 'UTF-8') \n",
    "# file_2 = file_1.read()\n",
    "\n",
    "def insert_to_dict(aDict,name, text_1):\n",
    "    if not name in aDict:\n",
    "        aDict[name] = [(\" \"+text_1)]\n",
    "    else:\n",
    "        aDict[name].append(\" \" + text_1)\n",
    "\n",
    "violence_dict = {'Bully_Data':[],'Drug_Data':[],'FA_Data':[],'SH_Data':[],'Vandalism_Data':[]}\n",
    "keys = ['Bully_Data','Drug_Data','FA_Data','SH_Data','Vandalism_Data']\n",
    "# violence_dict = {key: None for key in keys}\n",
    "\n",
    "for i in keys:\n",
    "    place = keys.index(i)\n",
    "    all_files = glob.glob(file_directory+file_folders[place]+\"/*.txt\")\n",
    "    for file in (all_files):\n",
    "        current_file = open(file,'r',encoding = 'latin-1').read()\n",
    "        insert_to_dict(violence_dict, i, current_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vandalism_Data [('school', 89), ('vandalism', 40), ('discipline', 35), ('butterworth,', 24), ('mayer,', 24)]\n",
      "Vandalism_Data [('school', 40), ('youngster', 15), ('student', 14), ('family', 13), ('cause', 12)]\n",
      "Vandalism_Data [('richmond', 22), ('man', 21), ('charged', 21), ('surrey', 16), ('also', 15)]\n",
      "Vandalism_Data [('school', 137), ('vandalism', 71), ('teacher', 70), ('student', 64), ('behavior', 64)]\n",
      "Vandalism_Data [('school', 50), ('vandalism', 33), ('toward', 27), ('level', 26), ('attitude', 23)]\n",
      "Vandalism_Data [('school', 181), ('student', 99), ('court', 71), ('board', 65), ('illinois', 46)]\n",
      "Vandalism_Data [('vandal', 84), ('child', 60), ('behavior', 50), ('activity', 48), ('vandalism', 39)]\n",
      "Vandalism_Data [('school', 398), ('vandalism', 161), ('community', 68), ('research', 64), ('may', 59)]\n",
      "Vandalism_Data [('residence', 166), ('hall', 107), ('size', 101), ('student', 77), ('vandalism', 75)]\n",
      "Vandalism_Data [('school', 22), ('individual', 22), ('sense', 19), ('act', 19), ('social', 15)]\n",
      "Bully_Data [('teacher', 152), ('student', 57), ('program', 53), ('school', 50), ('support', 38)]\n",
      "Bully_Data [('bullying', 46), ('social', 38), ('bully', 38), ('school', 36), ('peer', 31)]\n",
      "Bully_Data [('bullying', 45), ('student', 31), ('eric', 24), ('school', 19), ('bully', 13)]\n",
      "Bully_Data [('bullying', 124), ('child', 89), ('bullied', 69), ('school', 67), ('teacher', 48)]\n",
      "Bully_Data [('school', 112), ('student', 68), ('victim', 56), ('bullying', 54), ('bully', 43)]\n",
      "Bully_Data [('student', 95), ('conflict', 82), ('resolution', 62), ('school', 42), ('skill', 32)]\n",
      "Bully_Data [('term', 132), ('bullying', 88), ('cartoon', 62), ('cluster', 53), ('used', 49)]\n",
      "Bully_Data [('bullying', 303), ('child', 118), ('peer', 113), ('teacher', 81), ('may', 80)]\n",
      "Bully_Data [('peer', 173), ('bullying', 129), ('child', 59), ('bully', 47), ('school', 43)]\n",
      "Bully_Data [('school', 95), ('student', 86), ('bullying', 84), ('bullied', 55), ('cyberbullying', 47)]\n",
      "SH_Data [('sexual', 158), ('harassment', 112), ('student', 72), ('victimization', 67), ('perpetration', 55)]\n",
      "SH_Data [('sexual', 321), ('harassment', 174), ('youth', 142), ('gender', 67), ('victimization', 54)]\n",
      "SH_Data [('sexual', 214), ('harassment', 158), ('bullying', 148), ('school', 144), ('girl', 65)]\n",
      "SH_Data [('sexual', 138), ('sociosexual orientation inventory', 117), ('male', 77), ('female', 70), ('harassment', 70)]\n",
      "SH_Data [('physical relationship violence', 133), ('student', 94), ('bullying', 91), ('school', 85), ('victimization', 74)]\n",
      "SH_Data [('sexual', 130), ('harassment', 91), ('experienced', 43), ('participant', 38), ('school', 26)]\n",
      "SH_Data [('victimization', 159), ('school', 124), ('ethnic', 74), ('student', 73), ('ethnicity', 55)]\n",
      "SH_Data [('school', 205), ('climate', 105), ('class', 77), ('student', 47), ('study', 45)]\n",
      "SH_Data [('sexual', 159), ('harassment', 137), ('peer', 112), ('school', 104), ('study', 77)]\n",
      "SH_Data [('sexual', 321), ('harassment', 174), ('youth', 142), ('gender', 66), ('victimization', 54)]\n",
      "Drug_Data [('student', 110), ('substance', 93), ('school', 84), ('mandatory random student drug testing', 72), ('use', 66)]\n",
      "Drug_Data [('drug', 265), ('testing', 140), ('school', 118), ('use', 78), ('student', 48)]\n",
      "Drug_Data [('study', 77), ('patient', 68), ('intervention', 67), ('treatment', 64), ('psychiatric', 51)]\n",
      "Drug_Data [('bully', 167), ('bullying', 126), ('school', 82), ('student', 79), ('journal', 65)]\n",
      "Drug_Data [('bullying', 223), ('study', 164), ('victimization', 139), ('review', 132), ('effect', 125)]\n",
      "Drug_Data [('school', 55), ('drug', 48), ('student', 44), ('use', 33), ('used', 30)]\n",
      "Drug_Data [('alcohol', 105), ('sexual', 102), ('blackout', 75), ('drinking', 64), ('study', 58)]\n",
      "Drug_Data [('class', 64), ('use', 63), ('substance', 56), ('user', 41), ('alcohol', 40)]\n",
      "Drug_Data [('student', 203), ('testing', 190), ('school', 190), ('use', 122), ('random', 102)]\n",
      "Drug_Data [('student', 203), ('testing', 190), ('school', 190), ('use', 122), ('random', 102)]\n",
      "Drug_Data [('drug', 71), ('abuse', 56), ('sexual', 54), ('prescription', 52), ('initiation', 46)]\n",
      "Drug_Data [('drug', 155), ('bullying', 141), ('use', 117), ('study', 100), ('school', 98)]\n",
      "Drug_Data [('use', 91), ('drug', 69), ('user', 46), ('factor', 41), ('among', 29)]\n",
      "Drug_Data [('school', 48), ('youth', 35), ('drug', 34), ('lgbt', 34), ('use', 32)]\n",
      "Drug_Data [('lgbt', 141), ('substance', 119), ('treatment', 97), ('abuse', 78), ('sexual', 78)]\n",
      "FA_Data [('control', 56), ('risk', 52), ('injury', 50), ('factor', 43), ('group', 41)]\n",
      "FA_Data [('youth', 70), ('ed', 60), ('group', 31), ('violence', 28), ('assault-injuredyouth', 23)]\n",
      "FA_Data [('health', 23), ('child', 20), ('integrated management of childhood illness', 15), ('exposure', 14), ('mortality', 9)]\n",
      "FA_Data [('school', 158), ('assault', 49), ('teacher', 47), ('student', 35), ('physical', 32)]\n",
      "FA_Data [('injury', 68), ('among', 29), ('youth', 28), ('fight-relatedinjury', 26), ('adolescent', 26)]\n",
      "FA_Data [('behavior', 86), ('aggressive', 82), ('aggression', 77), ('child', 62), ('may', 61)]\n",
      "FA_Data [('problem', 95), ('social', 63), ('aggression', 61), ('solving', 38), ('found', 37)]\n",
      "FA_Data [('patient', 21), ('risk', 17), ('study', 13), ('within', 12), ('week', 11)]\n",
      "FA_Data [('physical', 56), ('aggression', 54), ('social', 41), ('risk', 37), ('school', 29)]\n",
      "FA_Data [('school', 166), ('violence', 90), ('student', 72), ('violent', 45), ('may', 38)]\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = ('& et et. al. = % 1 2 3 4 5 6 7 8 9 1â\\x81\\x844 â« - . ci: 95% â\\x80¢ (r').split()\n",
    "\n",
    "word_dict = {'Bully_Data':[],'Drug_Data':[],'FA_Data':[],'SH_Data':[],'Vandalism_Data':[]}\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for type_violence in violence_dict:\n",
    "    for paper in violence_dict[type_violence]:\n",
    "        words_1 = paper.lower().split()\n",
    "        \n",
    "        #Shorthand notation to longhand\n",
    "        for word in words_1:\n",
    "            if 'prv' in word:\n",
    "                index = words_1.index(word)\n",
    "                words_1[index] = 'physical relationship violence'\n",
    "            if 'soi' in word:\n",
    "                index = words_1.index(word)\n",
    "                words_1[index] = 'sociosexual orientation inventory'\n",
    "            if 'imci' in word:\n",
    "                index = words_1.index(word)\n",
    "                words_1[index] = 'integrated management of childhood illness'\n",
    "            if 'mrsdt' in word:\n",
    "                index = words_1.index(word)\n",
    "                words_1[index] = 'mandatory random student drug testing'\n",
    "            if 'gsa' in word:\n",
    "                index = words_1.index(word)\n",
    "                words_1[index] = 'gay-straight alliance'\n",
    "        \n",
    "        #Join words with hyphens\n",
    "        for word in words_1:\n",
    "            if '-' in word:\n",
    "                index = words_1.index(word)\n",
    "                index_1 = index +1 \n",
    "                words_1[index] = ''.join(words_1[index:index_1+1])\n",
    "        \n",
    "        #Stopword removal\n",
    "        filtered_words = [word for word in words_1 if word not in stopwords.words('english')]\n",
    "        filtered_words = [word for word in filtered_words if word not in custom_stopwords]\n",
    "        \n",
    "        #Remove words with periods in them\n",
    "        for word in filtered_words:\n",
    "            if '.' in word:\n",
    "                filtered_words.remove(word)\n",
    "                \n",
    "        #Remove words with commas in them\n",
    "        for word in filtered_words:\n",
    "            if ',' in word:\n",
    "                filtered_words.remove(word)\n",
    "        \n",
    "        #Remove words that are less than 2 letters long\n",
    "        for word in filtered_words:\n",
    "            if len(word) < 2:\n",
    "                filtered_words.remove(word)\n",
    "                \n",
    "        #Lemmatize\n",
    "        for word in filtered_words:\n",
    "            if word != wnl.lemmatize(word):\n",
    "                index = filtered_words.index(word)\n",
    "                filtered_words[index] = wnl.lemmatize(word)\n",
    "            \n",
    "        #Counter variable\n",
    "        wordcount = Counter(filtered_words)\n",
    "        print(type_violence, wordcount.most_common(5))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
